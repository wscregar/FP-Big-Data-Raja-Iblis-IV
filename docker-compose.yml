version: '3.8'

services:
  minio:
    image: bitnami/minio:latest
    container_name: minio
    ports:
      - "9000:9000"     # Web UI
      - "9001:9001"     # API port
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=minio123
      - BITNAMI_DEBUG=true
    volumes:
      - minio_data:/bitnami/minio/data
    command: minio server /bitnami/minio/data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - datanet

  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server kafka:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - datanet

  spark:
    image: bitnami/spark:latest
    container_name: spark
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_LOCAL_DIRS=/tmp/spark-local
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - IVY_HOME=/tmp/.ivy2
      - SPARK_HOME=/opt/bitnami/spark
      - PYSPARK_PYTHON=python3
      - USER=spark
      - HOME=/tmp
    ports:
      - "8080:8080"  # Spark UI
      - "7077:7077"  # Spark Master
      - "4040:4040"  # Spark Application UI
    networks:
      - datanet
    volumes:
      - ./spark:/app
      - spark_ivy:/tmp/.ivy2
    working_dir: /app
    command: >
      sh -c "
      echo 'Installing Python packages from requirements.txt...' &&
      pip install -r requirements.txt &&
      echo 'Python packages installed successfully!' &&
      echo 'Verifying py4j installation...' &&
      python3 -c 'import py4j; print(f\"py4j version: {py4j.__version__}\")' &&
      echo 'Starting Spark master...' &&
      /opt/bitnami/scripts/spark/entrypoint.sh /opt/bitnami/scripts/spark/run.sh &
      echo 'Waiting for Spark master to be ready...' &&
      sleep 30 &&
      echo 'Running ML training script...' &&
      python3 train_model.py
      "
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy

  streamlit:
    build: ./streamlit_app
    container_name: streamlit
    ports:
      - "8501:8501"
    networks:
      - datanet
    volumes:
      - ./streamlit_app:/app
    working_dir: /app
    command: streamlit run app.py
  
  producer:
    build: ./producer
    volumes:
      - ./data:/data
    networks:
      - datanet
    depends_on:
      kafka:
        condition: service_healthy
    restart: on-failure

  consumer:
    build: ./consumer
    networks:
      - datanet
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: on-failure


volumes:
  minio_data:
  spark_ivy:

networks:
  datanet:
    driver: bridge

